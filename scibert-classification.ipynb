{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (940212519.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    for dirname, _, filenames in os.walk('/kaggle/input'):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\\\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2177118512.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_path = \"C:\\Users\\User\\Documents\\Math Classification\\Code and Dataset\\train.json\"\u001b[0m\n\u001b[1;37m                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Paths to JSON files\n",
    "train_path = \"C:\\Users\\User\\Documents\\Math Classification\\Code and Dataset\\train.json\"\n",
    "test_path = \"C:\\Users\\User\\Documents\\Math Classification\\Code and Dataset\\test.json\"\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    \"Computations with Matrices\": 0,\n",
    "    \"Determinants\": 1,\n",
    "    \"Eigenvalues and Eigenvectors\": 2,\n",
    "    \"Linear Programming and Game Theory\": 3,\n",
    "    \"Matrices and Gaussian Elimination\": 4,\n",
    "    \"Orthogonality\": 5,\n",
    "    \"Positive Definite Matrices\": 6,\n",
    "    \"Vector Spaces\": 7\n",
    "}\n",
    "\n",
    "# Load train and test JSON\n",
    "def load_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        possible_keys = [\"data\", \"questions\", \"examples\"]\n",
    "        for key in possible_keys:\n",
    "            if key in data and isinstance(data[key], list):\n",
    "                data = data[key]\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"Expected list in {json_path}, but got dictionary without a valid key.\")\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(f\"Expected a list of dictionaries in {json_path}, but got {type(data)}.\")\n",
    "\n",
    "    texts, labels = [], []\n",
    "    \n",
    "    for item in data:\n",
    "        if not isinstance(item, dict):\n",
    "            raise ValueError(f\"Invalid item in JSON: {item}. Expected a dictionary.\")\n",
    "        if \"question_latex\" not in item or \"chapter\" not in item:\n",
    "            raise KeyError(f\"Missing required keys in JSON item: {item}\")\n",
    "\n",
    "        if item[\"chapter\"] not in label_mapping:\n",
    "            raise ValueError(f\"Unknown chapter '{item['chapter']}' in JSON file.\")\n",
    "\n",
    "        texts.append(item[\"question_latex\"])\n",
    "        labels.append(label_mapping[item[\"chapter\"]])  # Map chapter to label\n",
    "        \n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_data(train_path)\n",
    "test_texts, test_labels = load_data(test_path)\n",
    "\n",
    "# Load SciBERT tokenizer and model\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_mapping), ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"label\": train_labels\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "    \"label\": test_labels\n",
    "})\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./scibert_math_classification\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # Disables WandB logging if not used\n",
    ")\n",
    "\n",
    "# Trainer API\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save model & tokenizer\n",
    "model.save_pretrained(\"./scibert_math_model\")\n",
    "tokenizer.save_pretrained(\"./scibert_math_model\")\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6723839,
     "sourceId": 10828561,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
